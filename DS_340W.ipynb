{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO4P2aD3Ih8Q",
        "outputId": "0cf34860-d2b2-469c-cfa9-95c9d2e3f1d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('amazon.csv')\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if not word in stop_words]  # Remove stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]  # Lemmatize\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Apply text preprocessing\n",
        "data['preprocessed'] = data['review_content'].apply(preprocess_text)\n",
        "\n",
        "# POS tagging and extracting adjectives and adverbs\n",
        "def extract_adj_adv(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    return ' '.join([word for word, tag in tagged if tag in ['JJ', 'RB']])\n",
        "\n",
        "data['adj_adv_only'] = data['preprocessed'].apply(extract_adj_adv)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def compute_sentiment_score(text):\n",
        "    sentiment_scores = sia.polarity_scores(text)\n",
        "    return sentiment_scores['compound']  # Return the compound score\n",
        "\n",
        "data['sentiment_score'] = data['adj_adv_only'].apply(compute_sentiment_score)\n"
      ],
      "metadata": {
        "id": "kPkZJfSYKg0t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['adj_adv_only'])\n",
        "\n",
        "# Combine TF-IDF scores with sentiment scores\n",
        "tfidf_scores = tfidf_matrix.toarray()\n",
        "sentiment_scores = data['sentiment_score'].values\n",
        "\n",
        "# Calculate the product of TF-IDF scores and sentiment scores for each word in each review\n",
        "combined_scores = np.multiply(tfidf_scores, sentiment_scores[:, None])\n",
        "\n",
        "# Store terms and their combined scores in a DataFrame\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "combined_scores_mean = np.mean(combined_scores, axis=0)\n",
        "combined_df = pd.DataFrame({'Term': feature_names, 'Combined Score': combined_scores_mean})\n",
        "\n",
        "# Sort the DataFrame by combined scores\n",
        "combined_df_sorted = combined_df.sort_values(by='Combined Score', ascending=False)\n",
        "\n",
        "# Print the top 10 terms\n",
        "print(combined_df_sorted.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8zibNqaKimv",
        "outputId": "fa04c889-2359-4c92-b29a-b08ad26e18d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Term  Combined Score\n",
            "2280    good        0.112611\n",
            "185     also        0.036845\n",
            "1566    easy        0.035717\n",
            "6952    well        0.030486\n",
            "2377   great        0.026684\n",
            "5011  really        0.026653\n",
            "3864    nice        0.025657\n",
            "1900    fast        0.025623\n",
            "1738    even        0.022593\n",
            "1980    fine        0.021359\n"
          ]
        }
      ]
    }
  ]
}